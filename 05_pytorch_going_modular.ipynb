{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMp+dZm9usGP/u2OkfvTigT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HammadN98/pytorch/blob/main/05_pytorch_going_modular.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AU3mZmNJsKqq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### O CONTEUDO EM INGLES EH COPIA,  em portugues o feito neste modulo\n",
        "#### Copia ja q este modulo eh uma copia do passado, porem utilizando scripts python"
      ],
      "metadata": {
        "id": "oixwAm9wucqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "pytorch_project/\n",
        "├── pytorch_project/\n",
        "│   ├── data_setup.py\n",
        "│   ├── engine.py\n",
        "│   ├── model.py\n",
        "│   ├── train.py\n",
        "│   └── utils.py\n",
        "├── models/\n",
        "│   ├── model_1.pth\n",
        "│   └── model_2.pth\n",
        "└── data/\n",
        "    ├── data_folder_1/\n",
        "    └── data_folder_2/\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "LKKJWBEDtf5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* data_setup.py - um arquivo para preparar os dados (e baixar dados, se necessário).\n",
        "* engine.py - um arquivo contendo várias funções de treinamento.\n",
        "* model_builder.py ou model.py - um arquivo para criar um modelo PyTorch.\n",
        "* train.py - um arquivo para utilizar todos os outros arquivos e treinar um modelo PyTorch alvo.\n",
        "* utils.py - um arquivo dedicado a funções utilitárias úteis."
      ],
      "metadata": {
        "id": "faZh-6oBuNxt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we're going to cover\n",
        "By the end of this notebook you should finish with a directory structure of:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "going_modular/\n",
        "├── going_modular/\n",
        "│   ├── data_setup.py\n",
        "│   ├── engine.py\n",
        "│   ├── model_builder.py\n",
        "│   ├── train.py\n",
        "│   └── utils.py\n",
        "├── models/\n",
        "│   ├── 05_going_modular_cell_mode_tinyvgg_model.pth\n",
        "│   └── 05_going_modular_script_mode_tinyvgg_model.pth\n",
        "└── data/\n",
        "    └── pizza_steak_sushi/\n",
        "        ├── train/\n",
        "        │   ├── pizza/\n",
        "        │   │   ├── image01.jpeg\n",
        "        │   │   └── ...\n",
        "        │   ├── steak/\n",
        "        │   └── sushi/\n",
        "        └── test/\n",
        "            ├── pizza/\n",
        "            ├── steak/\n",
        "            └── sushi/\n",
        "```\n",
        "\n",
        "\n",
        "Using this directory structure, you should be able to train a model from within a notebook with the command:\n",
        "\n",
        "```!python going_modular/train.py```\n",
        "\n",
        "Or from the command line with:\n",
        "\n",
        "```python going_modular/train.py```\n",
        "\n",
        "In essence, we will have turned our helpful notebook code into reusable modular code."
      ],
      "metadata": {
        "id": "8TlSF8xlt5hZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Creating a folder for storing Python scripts\n",
        "Since we're going to be creating Python scripts out of our most useful code cells, let's create a folder for storing those scripts.\n",
        "\n",
        "We'll call the folder going_modular and create it using Python's os.makedirs() method."
      ],
      "metadata": {
        "id": "coerZUWHu5dV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"going_modular\", exist_ok=True)"
      ],
      "metadata": {
        "id": "GGlrv-9it-Hs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Get data\n",
        "We're going to start by downloading the same data we used in notebook 04, the pizza_steak_sushi dataset with images of pizza, steak and sushi."
      ],
      "metadata": {
        "id": "YgZuEhwDvA52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Download pizza, steak, sushi data\n",
        "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "    print(\"Downloading pizza, steak, sushi data...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "# Unzip pizza, steak, sushi data\n",
        "with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "    print(\"Unzipping pizza, steak, sushi data...\")\n",
        "    zip_ref.extractall(image_path)\n",
        "\n",
        "# Remove zip file\n",
        "os.remove(data_path / \"pizza_steak_sushi.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ud_vl24u9Rk",
        "outputId": "c5f6db4f-9e4f-490a-ace0-5d39cad13706"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not find data/pizza_steak_sushi directory, creating one...\n",
            "Downloading pizza, steak, sushi data...\n",
            "Unzipping pizza, steak, sushi data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setup train and testing paths\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\"\n",
        "\n",
        "train_dir, test_dir"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqsAfPisvGkP",
        "outputId": "20eb6a08-fc4a-44e9-80c7-3be8f45ece33"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(PosixPath('data/pizza_steak_sushi/train'),\n",
              " PosixPath('data/pizza_steak_sushi/test'))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Create Datasets and DataLoaders\n",
        "Let's turn our data into PyTorch Dataset's and DataLoader's and find out a few useful attributes from them such as classes and their lengths."
      ],
      "metadata": {
        "id": "oRe4UI7JvMLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Create simple transform\n",
        "data_transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Use ImageFolder to create dataset(s)\n",
        "train_data = datasets.ImageFolder(root=train_dir, # target folder of images\n",
        "                                  transform=data_transform, # transforms to perform on data (images)\n",
        "                                  target_transform=None) # transforms to perform on labels (if necessary)\n",
        "\n",
        "test_data = datasets.ImageFolder(root=test_dir,\n",
        "                                 transform=data_transform)\n",
        "\n",
        "print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_TX-4u-vJ3g",
        "outputId": "89d44c18-2acc-49b0-f7fe-1e8e06c461d6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 225\n",
            "    Root location: data/pizza_steak_sushi/train\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n",
            "Test data:\n",
            "Dataset ImageFolder\n",
            "    Number of datapoints: 75\n",
            "    Root location: data/pizza_steak_sushi/test\n",
            "    StandardTransform\n",
            "Transform: Compose(\n",
            "               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=True)\n",
            "               ToTensor()\n",
            "           )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get class names as a list\n",
        "class_names = train_data.classes\n",
        "class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_b0j3hYFvP1r",
        "outputId": "b129d18c-4210-477b-d222-d0533f99a840"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['pizza', 'steak', 'sushi']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can also get class names as a dict\n",
        "class_dict = train_data.class_to_idx\n",
        "class_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwygfnP3vTrO",
        "outputId": "38b8d796-e64e-4370-c928-be9fa2c512db"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'pizza': 0, 'steak': 1, 'sushi': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the lengths\n",
        "len(train_data), len(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0BYgpY-vTol",
        "outputId": "3697cf35-ed0a-43ff-e81d-974e0d5895d6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(225, 75)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn train and test Datasets into DataLoaders\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(dataset=train_data,\n",
        "                              batch_size=1, # how many samples per batch?\n",
        "                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n",
        "                              shuffle=True) # shuffle the data?\n",
        "\n",
        "test_dataloader = DataLoader(dataset=test_data,\n",
        "                             batch_size=1,\n",
        "                             num_workers=1,\n",
        "                             shuffle=False) # don't usually need to shuffle testing data\n",
        "\n",
        "train_dataloader, test_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYlFQsJCvTmi",
        "outputId": "12f9e5eb-1042-451e-88d8-7e2af59b9fce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7a99b91979a0>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7a99b9197c70>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out single image size/shape\n",
        "img, label = next(iter(train_dataloader))\n",
        "\n",
        "# Batch size will now be 1, try changing the batch_size parameter above and see what happens\n",
        "print(f\"Image shape: {img.shape} -> [batch_size, color_channels, height, width]\")\n",
        "print(f\"Label shape: {label.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cj7hXhkVvTj3",
        "outputId": "e32c031d-e6a8-40c0-9e63-48a83ac5aa1c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([1, 3, 64, 64]) -> [batch_size, color_channels, height, width]\n",
            "Label shape: torch.Size([1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Crianado o Dataset e Dataloaders com script"
      ],
      "metadata": {
        "id": "_228FT0Nve5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Crinaod o diretorio going_modular\n",
        "import os\n",
        "os.mkdirs(\"going_modular\")"
      ],
      "metadata": {
        "id": "wcC2x2a20hqn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "8d8c0bf9-6a4f-4664-ed0f-ba5a37ba238c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'os' has no attribute 'mkdirs'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a71fbdee718e>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Crinaod o diretorio going_modular\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"going_modular\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'os' has no attribute 'mkdirs'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/data_setup.py\n",
        "\n",
        "\"\"\"\n",
        "Contem as funcionalidades necessarias para criar Dataloaders para prblemas de classificacao de imagens\n",
        "\"\"\"\n",
        "import os\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "NUM_WORKERS = os.cpu_count()\n",
        "\n",
        "def create_dataloaders(\n",
        "  train_dir: str,\n",
        "  test_dir: str,\n",
        "  transform: transforms.Compose,\n",
        "  batch_size: int,\n",
        "  num_workers=NUM_WORKERS,\n",
        "):\n",
        "  \"\"\"Cria os DataLoaders de treino e teste\n",
        "\n",
        "  Recebe um caminho para diretorio de treino e um para teste e transforma-os para Pytorch Datasets e depois em Pytorch DataLoaders\n",
        "\n",
        "  Args:\n",
        "    train_dir: caminho para o diretorio de treino.\n",
        "    test_dir: caminho para o diretorio de teste.\n",
        "    transform: torchvision transforms para transformar os dados de treino e teste.\n",
        "    batch_size: numero de amostras por batch em cada dataloader\n",
        "    num_workers: Numero inteiro de trabalhadores por DataLoader.\n",
        "\n",
        "  Returns:\n",
        "    Uma tupla de (tran_dataloader, test_dataloader, class_names)\n",
        "    Aonde class_names eh uma lista das classes alvos\n",
        "    Exemplo de uso:\n",
        "      train_dataloader, test_dataloader, class_names = create_dataloaders(train_dir=path/to/train_dir,\n",
        "        test_dir=pat/to/test_dir,\n",
        "        transform=some_transform,\n",
        "        batch_size=32,\n",
        "        num_workers=NUM_WORKERS)\n",
        "  \"\"\"\n",
        "  train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
        "  test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "  #NOmes das classes\n",
        "  class_names = train_data.classes\n",
        "\n",
        "  #TRansformando em dataloaders\n",
        "  train_dataloader = DataLoader(\n",
        "      train_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=True,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  test_dataloader = DataLoader(\n",
        "      test_data,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=False,\n",
        "      num_workers=num_workers,\n",
        "      pin_memory=True\n",
        "  )\n",
        "\n",
        "  return train_dataloader, test_dataloader, class_names\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlqltANhvTh2",
        "outputId": "30b3fdd5-1a42-4fe8-f2ef-37edb3821f42"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/data_setup.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular import data_setup\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                               test_dir=test_dir,\n",
        "                                                                               transform=data_transform,\n",
        "                                                                               batch_size=32)\n",
        "train_dataloader, test_dataloader, class_names"
      ],
      "metadata": {
        "id": "QnzfuQRwvTfE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "162323a1-0d8b-4082-8d3c-596542c12cb8"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<torch.utils.data.dataloader.DataLoader at 0x7a98f4af9990>,\n",
              " <torch.utils.data.dataloader.DataLoader at 0x7a98f4afa170>,\n",
              " ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Making a model (TinyVGG)\n",
        "We're going to use the same model we used in notebook 04: TinyVGG from the CNN Explainer website.\n",
        "\n",
        "The only change here from notebook 04 is that a docstring has been added using Google's Style Guide for Python."
      ],
      "metadata": {
        "id": "UAh9IeQhypIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "    Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "id": "VjFtBX1EBHD4"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate an instance of the model\n",
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZVnmtY1yulo",
        "outputId": "a54b6794-cda5-4dd6-ae68-fa64a469aaea"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyVGG(\n",
              "  (conv_block_1): Sequential(\n",
              "    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (conv_block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=1690, out_features=3, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check out our model by doing a dummy forward pass."
      ],
      "metadata": {
        "id": "Vh6xZWa5y0gP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Get a batch of images and labels from the DataLoader\n",
        "img_batch, label_batch = next(iter(train_dataloader))\n",
        "\n",
        "# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\n",
        "img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\n",
        "print(f\"Single image shape: {img_single.shape}\\n\")\n",
        "\n",
        "# 3. Perform a forward pass on a single image\n",
        "model_0.eval()\n",
        "with torch.inference_mode():\n",
        "    pred = model_0(img_single.to(device))\n",
        "\n",
        "# 4. Print out what's happening and convert model logits -> pred probs -> pred label\n",
        "print(f\"Output logits:\\n{pred}\\n\")\n",
        "print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\n",
        "print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\n",
        "print(f\"Actual label:\\n{label_single}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3QYdrbsy07m",
        "outputId": "8134eb37-8993-4c04-b7e1-c86b37d13834"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single image shape: torch.Size([1, 3, 64, 64])\n",
            "\n",
            "Output logits:\n",
            "tensor([[ 0.0208, -0.0020,  0.0095]])\n",
            "\n",
            "Output prediction probabilities:\n",
            "tensor([[0.3371, 0.3295, 0.3333]])\n",
            "\n",
            "Output prediction label:\n",
            "tensor([0])\n",
            "\n",
            "Actual label:\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Criando um modelo (TinyVGG) com script (`model_builder.py`)\n",
        "\n",
        "Vamos transformar o construtor do modelo em um script python, que possa ser importando\n",
        "\n"
      ],
      "metadata": {
        "id": "kYd57_iDzSn7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile going_modular/model_builder.py\n",
        "\"\"\"\n",
        "Contem um modelo Pytorch para instanciar a rede TinyVGG do CNN Explainer Website.\n",
        "\"\"\"\n",
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "\n",
        "class TinyVGG(nn.Module):\n",
        "    \"\"\"Creates the TinyVGG architecture.\n",
        "\n",
        "    Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n",
        "    See the original architecture here: https://poloclub.github.io/cnn-explainer/\n",
        "\n",
        "    Args:\n",
        "    input_shape: An integer indicating number of input channels.\n",
        "    hidden_units: An integer indicating number of hidden units between layers.\n",
        "    output_shape: An integer indicating number of output units.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
        "        super().__init__()\n",
        "        self.conv_block_1 = nn.Sequential(\n",
        "          nn.Conv2d(in_channels=input_shape,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(in_channels=hidden_units,\n",
        "                    out_channels=hidden_units,\n",
        "                    kernel_size=3,\n",
        "                    stride=1,\n",
        "                    padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(kernel_size=2,\n",
        "                        stride=2)\n",
        "        )\n",
        "        self.conv_block_2 = nn.Sequential(\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n",
        "          nn.ReLU(),\n",
        "          nn.MaxPool2d(2)\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "          nn.Flatten(),\n",
        "          # Where did this in_features shape come from?\n",
        "          # It's because each layer of our network compresses and changes the shape of our inputs data.\n",
        "          nn.Linear(in_features=hidden_units*13*13,\n",
        "                    out_features=output_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = self.conv_block_1(x)\n",
        "        x = self.conv_block_2(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "        # return self.classifier(self.block_2(self.block_1(x))) # <- leverage the benefits of operator fusion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dp_jT6IUze4Y",
        "outputId": "dfa04d33-7c01-4bd2-e517-03775b5d4df5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing going_modular/model_builder.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Instantiate an instance of the model\n",
        "torch.manual_seed(42)\n",
        "model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)\n",
        "                  hidden_units=10,\n",
        "                  output_shape=len(train_data.classes)).to(device)\n",
        "model_0"
      ],
      "metadata": {
        "id": "5zRUb3lo0pPR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}