{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPCImaKPf3/So0TejN00mJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6f86eb339afa485085060e5b0153a89a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4a86b63a4914f6ebb3a9a98ed432868",
              "IPY_MODEL_650701c7d55f4050921d71617c1b2b75",
              "IPY_MODEL_8da957b166464e739680e264a9afb6bd"
            ],
            "layout": "IPY_MODEL_db355fa7c0f54efc914f45ccf68f91e6"
          }
        },
        "f4a86b63a4914f6ebb3a9a98ed432868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4b04ae22b194ba5a02c5485a3c48795",
            "placeholder": "​",
            "style": "IPY_MODEL_2613a2874c454638992a13afc4023b48",
            "value": "  0%"
          }
        },
        "650701c7d55f4050921d71617c1b2b75": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f918c9de67e04a81b06856fe3932cb34",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1548c8864c647569ca584246fcafbd9",
            "value": 0
          }
        },
        "8da957b166464e739680e264a9afb6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b2f313b9f81441f48ddd2acace8e4d6e",
            "placeholder": "​",
            "style": "IPY_MODEL_3f8ef2dddd274d00b0994261e1c2348e",
            "value": " 0/10 [00:00&lt;?, ?it/s]"
          }
        },
        "db355fa7c0f54efc914f45ccf68f91e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4b04ae22b194ba5a02c5485a3c48795": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2613a2874c454638992a13afc4023b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f918c9de67e04a81b06856fe3932cb34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1548c8864c647569ca584246fcafbd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b2f313b9f81441f48ddd2acace8e4d6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f8ef2dddd274d00b0994261e1c2348e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HammadN98/pytorch/blob/main/09_pytorch_deploy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 09. Deploy de Modelow\n",
        "\n",
        "Pra q serve?\n",
        "\n",
        "Para deixar o codigo que esta no notebook em algo usavel por outros/outra coisa.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nb2AXEZbN9Ez"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Importando os basicos\n",
        "\n",
        "* Pacotes\n",
        "* E baixandos o going_modular do github"
      ],
      "metadata": {
        "id": "bgKDHJQ-URi9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ceN6d9m5NbUj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecbccb1d-2340-446d-f7e2-cfd4fe2cca95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Couldn't find torchinfo... installing it.\n",
            "[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\n",
            "Cloning into 'pytorch-deep-learning'...\n",
            "remote: Enumerating objects: 4356, done.\u001b[K\n",
            "remote: Counting objects: 100% (321/321), done.\u001b[K\n",
            "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
            "remote: Total 4356 (delta 213), reused 254 (delta 178), pack-reused 4035 (from 1)\u001b[K\n",
            "Receiving objects: 100% (4356/4356), 654.51 MiB | 33.25 MiB/s, done.\n",
            "Resolving deltas: 100% (2585/2585), done.\n",
            "Updating files: 100% (248/248), done.\n"
          ]
        }
      ],
      "source": [
        "#ctrl c + ctrl v dos imports pardrao\n",
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "    from helper_functions import download_data, set_seeds, plot_loss_curves"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ake4kL1wQoSb",
        "outputId": "915ddd7a-13f5-47ad-f032-848f53e2d403"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Baixando os dados"
      ],
      "metadata": {
        "id": "IBKCA-EvUJub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ctrl c + ctrl v dos imports pardrao\n",
        "# Download pizza, steak, sushi images from GitHub\n",
        "data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n",
        "                                     destination=\"pizza_steak_sushi_20_percent\")\n",
        "\n",
        "data_20_percent_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhxa56TJQxzU",
        "outputId": "75a3e3db-880c-45d6-9aa0-f9ef672814d8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Did not find data/pizza_steak_sushi_20_percent directory, creating one...\n",
            "[INFO] Downloading pizza_steak_sushi_20_percent.zip from https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip...\n",
            "[INFO] Unzipping pizza_steak_sushi_20_percent.zip data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PosixPath('data/pizza_steak_sushi_20_percent')"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ctrl c + ctrl v dos imports pardrao\n",
        "# Setup directory paths to train and test images\n",
        "train_dir = data_20_percent_path / \"train\"\n",
        "test_dir = data_20_percent_path / \"test\""
      ],
      "metadata": {
        "id": "TqZZ4MEgQ1Br"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Experimentos para o Deploy do FoodVisiob Mini\n",
        "\n",
        "### 3 Perguntas a serem respondidas?\n",
        "\n",
        "1. QUal o melhor cenario para o deploy desse modeelo de machine learning?\n",
        "2. Para aondemeu modelo ira?\n",
        "3. Como o modelo ira funcionar?\n",
        "\n",
        "**USo ideal do FoodVision Mini:**  Um modelo que performa bem e seeja rapido\n",
        "  1. PErformace: 95%+ de acurácia\n",
        "  2. Rapdo: O mais perto de real-time possivel (30FPS+ ou 30ms de latencia)\n",
        "\n",
        "Para conseguir sses objetivos serao realizados eexperimentos em dois modelos:\n",
        "  1. EffNetBB2\n",
        "  2. ViT  "
      ],
      "metadata": {
        "id": "Qc0kVfZ5Xog_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Criando um extrator de caracteristicas EffNetB2\n",
        "\n",
        "extrator de caracteristicas = Um termo para \"transer learning\", aonde o modelo tem suas camadas bases congeladas e saida customizada para o problema em questao."
      ],
      "metadata": {
        "id": "m7ipDoIhmLgt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#PEgando os pesos\n",
        "effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "\n",
        "#PEgando os transforms\n",
        "efnetb2_transforms = effnetb2_weights.transforms()\n",
        "\n",
        "#Instanciando um model effnet com os pesos DEFAULT\n",
        "effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n",
        "\n",
        "#COngelando as camadas baes\n",
        "for param in effnetb2.parameters():\n",
        " param.requires_grad=False"
      ],
      "metadata": {
        "id": "DUisoiiBqgRG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb11d637-e18c-4150-a90d-fada7fb058fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/efficientnet_b2_rwightman-c35c1473.pth\" to /root/.cache/torch/hub/checkpoints/efficientnet_b2_rwightman-c35c1473.pth\n",
            "100%|██████████| 35.2M/35.2M [00:02<00:00, 17.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        " # Print EffNetB2 model summary (uncomment for full output)\n",
        "saidas = summary(effnetb2,\n",
        "         input_size=(1, 3, 224, 224),\n",
        "         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "         col_width=20,\n",
        "         row_settings=[\"var_names\"])\n",
        "#saidas"
      ],
      "metadata": {
        "id": "oVXSO7ZmqgO9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2.classifier"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GJmQ1H4T3Wf",
        "outputId": "a6c42e29-1cbe-4579-9a98-7d4d934af299"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): Dropout(p=0.3, inplace=True)\n",
              "  (1): Linear(in_features=1408, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "set_seeds()\n",
        "effnetb2.classifier = nn.Sequential(nn.Dropout(p=0.3, inplace=True),\n",
        "                                    nn.Linear(in_features=1408, out_features=3))"
      ],
      "metadata": {
        "id": "ILBU2ISrT4Aj"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Print EffNetB2 model summary (uncomment for full output)\n",
        "saidas2 = summary(effnetb2,\n",
        "         input_size=(1, 3, 224, 224),\n",
        "         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "         col_width=20,\n",
        "         row_settings=[\"var_names\"])\n",
        "saidas2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3XcMTvZsjQR",
        "outputId": "6ebd64fc-80a2-4c49-cb7a-7b8f05f68506"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   Partial\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   False\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    (864)                False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    (64)                 False\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    (1,448)              False\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    (612)                False\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      (6,004)              False\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      (10,710)             False\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      (16,518)             False\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      (43,308)             False\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      (50,300)             False\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      (123,750)            False\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     (149,158)            False\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     (237,870)            False\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       (301,406)            False\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       (686,868)            False\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   False\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       (846,900)            False\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       (1,888,920)          False\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   False\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      (495,616)            False\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      (2,816)              False\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 4,227\n",
              "Non-trainable params: 7,700,994\n",
              "Total mult-adds (M): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Criando uma funcao para criar um modelo de extracao de caracteristicas EffNetB2"
      ],
      "metadata": {
        "id": "G5RH-M9IRNfg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_effnetb2(num_classes:int = 3,\n",
        "                    seed:int=3):\n",
        "\n",
        "  # Pesos, transforms, e instanciando o modelo\n",
        "  weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n",
        "  transforms = weights.transforms()\n",
        "  model = torchvision.models.efficientnet_b2(weights=weights)\n",
        "\n",
        "  #COngelando as camdas base\n",
        "  for param in model.parameters():\n",
        "    requires_grad=False\n",
        "\n",
        "  #AJustando o classifier para o problema em questao\n",
        "  torch.manual_seed(seed)\n",
        "  model.classifier = nn.Sequential(\n",
        "      nn.Dropout(p=0.3, inplace=True),\n",
        "      nn.Linear(in_features=1408, out_features=3))\n",
        "\n",
        "  return model, transforms"
      ],
      "metadata": {
        "id": "j-GcfyS2RhHo"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2, effnetb2_transforms = create_effnetb2(num_classes=3, seed=42)"
      ],
      "metadata": {
        "id": "0PHavhL5Sljy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_transforms"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CpGT6sFhSxOj",
        "outputId": "31648649-f00b-4c03-e78a-4b8094c74789"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ImageClassification(\n",
              "    crop_size=[288]\n",
              "    resize_size=[288]\n",
              "    mean=[0.485, 0.456, 0.406]\n",
              "    std=[0.229, 0.224, 0.225]\n",
              "    interpolation=InterpolationMode.BICUBIC\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(effnetb2,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tW00X18dTP6e",
        "outputId": "f8a0fef3-b0c4-4423-e9bf-6b765f2da199"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "EfficientNet (EfficientNet)                                  [1, 3, 224, 224]     [1, 3]               --                   True\n",
              "├─Sequential (features)                                      [1, 3, 224, 224]     [1, 1408, 7, 7]      --                   True\n",
              "│    └─Conv2dNormActivation (0)                              [1, 3, 224, 224]     [1, 32, 112, 112]    --                   True\n",
              "│    │    └─Conv2d (0)                                       [1, 3, 224, 224]     [1, 32, 112, 112]    864                  True\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 32, 112, 112]    [1, 32, 112, 112]    64                   True\n",
              "│    │    └─SiLU (2)                                         [1, 32, 112, 112]    [1, 32, 112, 112]    --                   --\n",
              "│    └─Sequential (1)                                        [1, 32, 112, 112]    [1, 16, 112, 112]    --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 32, 112, 112]    [1, 16, 112, 112]    1,448                True\n",
              "│    │    └─MBConv (1)                                       [1, 16, 112, 112]    [1, 16, 112, 112]    612                  True\n",
              "│    └─Sequential (2)                                        [1, 16, 112, 112]    [1, 24, 56, 56]      --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 16, 112, 112]    [1, 24, 56, 56]      6,004                True\n",
              "│    │    └─MBConv (1)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      10,710               True\n",
              "│    │    └─MBConv (2)                                       [1, 24, 56, 56]      [1, 24, 56, 56]      10,710               True\n",
              "│    └─Sequential (3)                                        [1, 24, 56, 56]      [1, 48, 28, 28]      --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 24, 56, 56]      [1, 48, 28, 28]      16,518               True\n",
              "│    │    └─MBConv (1)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      43,308               True\n",
              "│    │    └─MBConv (2)                                       [1, 48, 28, 28]      [1, 48, 28, 28]      43,308               True\n",
              "│    └─Sequential (4)                                        [1, 48, 28, 28]      [1, 88, 14, 14]      --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 48, 28, 28]      [1, 88, 14, 14]      50,300               True\n",
              "│    │    └─MBConv (1)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
              "│    │    └─MBConv (2)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
              "│    │    └─MBConv (3)                                       [1, 88, 14, 14]      [1, 88, 14, 14]      123,750              True\n",
              "│    └─Sequential (5)                                        [1, 88, 14, 14]      [1, 120, 14, 14]     --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 88, 14, 14]      [1, 120, 14, 14]     149,158              True\n",
              "│    │    └─MBConv (1)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
              "│    │    └─MBConv (2)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
              "│    │    └─MBConv (3)                                       [1, 120, 14, 14]     [1, 120, 14, 14]     237,870              True\n",
              "│    └─Sequential (6)                                        [1, 120, 14, 14]     [1, 208, 7, 7]       --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 120, 14, 14]     [1, 208, 7, 7]       301,406              True\n",
              "│    │    └─MBConv (1)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
              "│    │    └─MBConv (2)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
              "│    │    └─MBConv (3)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
              "│    │    └─MBConv (4)                                       [1, 208, 7, 7]       [1, 208, 7, 7]       686,868              True\n",
              "│    └─Sequential (7)                                        [1, 208, 7, 7]       [1, 352, 7, 7]       --                   True\n",
              "│    │    └─MBConv (0)                                       [1, 208, 7, 7]       [1, 352, 7, 7]       846,900              True\n",
              "│    │    └─MBConv (1)                                       [1, 352, 7, 7]       [1, 352, 7, 7]       1,888,920            True\n",
              "│    └─Conv2dNormActivation (8)                              [1, 352, 7, 7]       [1, 1408, 7, 7]      --                   True\n",
              "│    │    └─Conv2d (0)                                       [1, 352, 7, 7]       [1, 1408, 7, 7]      495,616              True\n",
              "│    │    └─BatchNorm2d (1)                                  [1, 1408, 7, 7]      [1, 1408, 7, 7]      2,816                True\n",
              "│    │    └─SiLU (2)                                         [1, 1408, 7, 7]      [1, 1408, 7, 7]      --                   --\n",
              "├─AdaptiveAvgPool2d (avgpool)                                [1, 1408, 7, 7]      [1, 1408, 1, 1]      --                   --\n",
              "├─Sequential (classifier)                                    [1, 1408]            [1, 3]               --                   True\n",
              "│    └─Dropout (0)                                           [1, 1408]            [1, 1408]            --                   --\n",
              "│    └─Linear (1)                                            [1, 1408]            [1, 3]               4,227                True\n",
              "============================================================================================================================================\n",
              "Total params: 7,705,221\n",
              "Trainable params: 7,705,221\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 657.64\n",
              "============================================================================================================================================\n",
              "Input size (MB): 0.60\n",
              "Forward/backward pass size (MB): 156.80\n",
              "Params size (MB): 30.82\n",
              "Estimated Total Size (MB): 188.22\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COnfigurando Dataloaders pra o EffNetB2\n"
      ],
      "metadata": {
        "id": "c_xH5NBxUYGF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import data_setup\n",
        "\n",
        "train_datalaoder_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                                 test_dir=test_dir,\n",
        "                                                                                                 transform=effnetb2_transforms,\n",
        "                                                                                                 batch_size=32)"
      ],
      "metadata": {
        "id": "H2jZUPC-Uu3E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_datalaoder_effnetb2), len(test_dataloader_effnetb2), class_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1qWbw7FUzud",
        "outputId": "b5dca33b-64e3-4651-9bf2-6b45ab7f3243"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15, 5, ['pizza', 'steak', 'sushi'])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Treinado o extrator de caracteristicas EffNetB2"
      ],
      "metadata": {
        "id": "sLqNKCyvVYCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import engine\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(effnetb2.parameters(), lr=1e-3)\n",
        "\n",
        "modelo = engine.train(model=effnetb2,\n",
        "                      train_dataloader=train_datalaoder_effnetb2,\n",
        "                      test_dataloader=test_dataloader_effnetb2,\n",
        "                      optimizer=optimizer,\n",
        "                      loss_fn=loss_fn,\n",
        "                      epochs=10,\n",
        "                      device=device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "6f86eb339afa485085060e5b0153a89a",
            "f4a86b63a4914f6ebb3a9a98ed432868",
            "650701c7d55f4050921d71617c1b2b75",
            "8da957b166464e739680e264a9afb6bd",
            "db355fa7c0f54efc914f45ccf68f91e6",
            "f4b04ae22b194ba5a02c5485a3c48795",
            "2613a2874c454638992a13afc4023b48",
            "f918c9de67e04a81b06856fe3932cb34",
            "a1548c8864c647569ca584246fcafbd9",
            "b2f313b9f81441f48ddd2acace8e4d6e",
            "3f8ef2dddd274d00b0994261e1c2348e"
          ]
        },
        "id": "KHTcIx02VWkg",
        "outputId": "171485da-3a97-4a7f-ccb6-4cb232f35255"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f86eb339afa485085060e5b0153a89a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 Inspecionando os curvas da EffNetB2\n"
      ],
      "metadata": {
        "id": "rMIv78RjYNLi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from helper_functions import plot_loss_curves\n",
        "plot_loss_curves(modelo)"
      ],
      "metadata": {
        "id": "iUWk4SSvVwsP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 Salvando o modelo EffNetB2\n"
      ],
      "metadata": {
        "id": "vJayJP-ad3p1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from going_modular.going_modular import utils\n",
        "\n",
        "utils.save_model(model=effnetb2,\n",
        "                 target_dir='models',\n",
        "                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "hoa5j3hbYVd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 Descobrindo o tamanho do arquivo do modelo\n",
        "\n",
        "Pq isso e importante?\n",
        "\n",
        "Pq estamos lancando esse modelo pra ser usado em um app/mobile que pode ter limitacoes de recurcos computacionais. Assim se o modelo for muit grande (para os dispositivos em questao) ele pode nao rodar/ ter problemas ao rodar.\n"
      ],
      "metadata": {
        "id": "Is7_4Yf2fABH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "#Pegando o tamanho em bytes e convertendo para mega\n",
        "pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024)\n",
        "print(f\"Tamanho do modelo {round(pretrained_effnetb2_model_size, 2)}MB\")"
      ],
      "metadata": {
        "id": "X1EYz9eueVpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 Estatisticas do extrator de caracteristicas da EffNetB2"
      ],
      "metadata": {
        "id": "bL08bq5Alflo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Contagem da quantidade de parametros\n",
        "effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\n",
        "effnetb2_total_params"
      ],
      "metadata": {
        "id": "McR60seXhB9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando um dicionarios com os resultados do treinamento do modelo para posterior comparacao\n",
        "effnetb2_stats = {\"test_loss\": modelo[\"test_loss\"][-1],\n",
        "                  \"test_acc\": modelo[\"test_acc\"][-1],\n",
        "                  \"number_of_parameters\": effnetb2_total_params,\n",
        "                  \"model_size (MB)\": pretrained_effnetb2_model_size}\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "vgi8DjP-mBhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Criando um extrator de caracteristicas usando o ViT"
      ],
      "metadata": {
        "id": "JccMiTiBmygS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "\n",
        "vit_transforms = vit_weights.transforms()\n",
        "\n",
        "vit = torchvision.models.vit_b_16(weights=vit_weights)"
      ],
      "metadata": {
        "id": "U-l29_QDmubY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for param in vit.parameters():\n",
        "  requires_grad=False"
      ],
      "metadata": {
        "id": "X_M49hWRnTQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "iX3eiJk6ntCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.heads"
      ],
      "metadata": {
        "id": "-POSr2EDnjaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit.heads = nn.Sequential(\n",
        "    nn.Linear(in_features=768, out_features=3)\n",
        ")"
      ],
      "metadata": {
        "id": "KMsSUxxjnm15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "i7eOiWCZoDla"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_vit(out_features:int=3, seed:int=42):\n",
        "  vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
        "\n",
        "  vit_transforms = vit_weights.transforms()\n",
        "\n",
        "  vit = torchvision.models.vit_b_16(weights=vit_weights)\n",
        "\n",
        "  for param in vit.parameters():\n",
        "    param.requires_grad=False\n",
        "\n",
        "  vit.heads = nn.Sequential(\n",
        "    nn.Linear(in_features=768, out_features=3)\n",
        "    )\n",
        "\n",
        "  return vit, vit_transforms"
      ],
      "metadata": {
        "id": "b8ZRe5dAokQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit, vit_transform = create_vit(out_features=3,\n",
        "           seed=42)\n",
        "vit_transform"
      ],
      "metadata": {
        "id": "YyLJZE9So9kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary(vit,\n",
        "        input_size=(1, 3, 224, 224),\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"])"
      ],
      "metadata": {
        "id": "IxeOat3ioHk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 Criando os DataLoaders para a ViT"
      ],
      "metadata": {
        "id": "liv9n0UbiCWz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_vit_dataloader, test_vit_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n",
        "                                                                                       test_dir=test_dir,\n",
        "                                                                                       transform=vit_transform,\n",
        "                                                                                       batch_size=32)\n",
        "len(train_vit_dataloader), len(test_vit_dataloader), class_names"
      ],
      "metadata": {
        "id": "D152Tam9pYzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 Treinando a ViT"
      ],
      "metadata": {
        "id": "3HkkmLmLjuer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(vit.parameters(), lr=1e-3)\n"
      ],
      "metadata": {
        "id": "6X7ymiBfq5NB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_results = engine.train(model=vit,\n",
        "                           train_dataloader=train_vit_dataloader,\n",
        "                           test_dataloader=test_vit_dataloader,\n",
        "                           optimizer=optimizer,\n",
        "                           loss_fn=loss_fn,\n",
        "                           epochs=10,\n",
        "                           device=device)"
      ],
      "metadata": {
        "id": "mP_EeO78rLnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.3 Plotando as curvas do treinamento"
      ],
      "metadata": {
        "id": "9w6P5CmTk7C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curves(vit_results)"
      ],
      "metadata": {
        "id": "nOGwtGPfrfBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 Salvando a ViT"
      ],
      "metadata": {
        "id": "Qy2AtHEll76X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "utils.save_model(model=vit,\n",
        "                 target_dir='models',\n",
        "                model_name=\"09_pretrained_vit_feature_extrator_pizza_steak_sushi_20_percent.pth\")"
      ],
      "metadata": {
        "id": "6ZZAAq00rqEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 Checando o tamanho do modelo ViT"
      ],
      "metadata": {
        "id": "QQ_fpu59mxC_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tamanho_modelo= Path(\"models/09_pretrained_vit_feature_extrator_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024)\n",
        "print(f\" size: {tamanho_modelo} mb\")\n"
      ],
      "metadata": {
        "id": "qc1aKob8sRv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.6 Estatisticas do modelo ViT"
      ],
      "metadata": {
        "id": "-LZgMwEgm4Pt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Contagem da quantidade de parametros\n",
        "vit_total_params = sum(torch.numel(param) for param in vit.parameters())\n",
        "vit_total_params"
      ],
      "metadata": {
        "id": "aShzbRp2tzsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_stats = {\"test_loss\" : vit_results[\"test_loss\"][-1],\n",
        "             \"test_acc\" : vit_results[\"test_acc\"][-1],\n",
        "             \"number_of_parameters\" : vit_total_params,\n",
        "             \"model_size (MB)\": tamanho_modelo\n",
        "             }\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "b7S_zrJdsMmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Realizando predicoes com os modelos treinados e conometrando\n",
        "\n",
        "Objetivos\n",
        "  * 95%+ Acc\n",
        "  * 30+ FPS\n",
        "\n",
        "Criterios do teste:\n",
        "\n",
        "1. Passar pelas imagens de teste\n",
        "2. Calcular quanto tempo cada modelo leva para realizar a predicao na imagem"
      ],
      "metadata": {
        "id": "p7ttXjX1sKik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "#PPEgando todos os aminhos dos dadoss de teste\n",
        "test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_paths[:5]"
      ],
      "metadata": {
        "id": "n7RQiEmBoLU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.1 Criando a funcao para fazer teste em todo dataset\n",
        "\n",
        "* `pred_and_store()`\n",
        "\n",
        "1. Criar um funcao que receba uma lista de caminhos, um modelo pytorch, um transforms, lista das classes alvo, e um dispositivo.\n",
        "2. Criar uma lista vazia, que ira receber todas as predicoes\n",
        "3. PErcorrer sobre os caminhos dos dados (os proximos passos ocorreram dentro dest loop).\n",
        "4. Criar um dicionario vazio para cada amostra (As stats de predicao irao aqui)\n",
        "5. Pegar o caminho da amostra e a classe verdadeira do filepath\n",
        "6. Comecar o tempo da predicao\n",
        "7.  Abrir a imagem usando `PIL.Image.open(path)`.\n",
        "8. Transformar a imagem para estar apta ao modelo.\n",
        "9. Preparar o modelo para inferencia, mandar para o dispositivo alvo e acionar o modo `eval()`.\n",
        "10. Ligar o torch.inference_mode() e passar a imagem alvo transformada para o modelo e performar o forward pass + calculo da pred prob + pred class.\n",
        "11. Adicionar a pred prob + pred class ao dicionario do passo 4.\n",
        "12. Encerrrar o timer, e adicionar o tempo decorrido ao dicionario.\n",
        "13. Verificar se as classes preditas sao as verdadeiras.\n",
        "14. Adiconar ao fim da lista as predicoes conforme vao ocorrendo\n",
        "15. Devolver a list de dicionarios de predicoes.\n"
      ],
      "metadata": {
        "id": "jkyfTr_eq5gI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List\n",
        "\n",
        "# 1.\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "  #2.\n",
        "  pred_list = []\n",
        "\n",
        "  #3.\n",
        "  for path in tqdm(paths):\n",
        "\n",
        "    # 4.\n",
        "    pred_dict = {}\n",
        "\n",
        "    # 5.\n",
        "    pred_dict[\"image_path\"] = path\n",
        "    class_names = path.parent.stem\n",
        "    pred_dict[\"class_names\"] = class_names\n",
        "\n",
        "    # 6.\n",
        "    start_time = timer()\n",
        "\n",
        "    # 7.\n",
        "    img = Image.open(path)\n",
        "\n",
        "    # 8.\n",
        "    transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    # 9.\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # 10.\n",
        "    with torch.inference_mode():\n",
        "      pred_logit = model(transformed_image)\n",
        "      pred_prob = torch.softmax(pred_logit, dim=1)\n",
        "      pred_label = torch.argmax(pred_prob, dim=1)\n",
        "      pred_class = class_names[pred_label.cpu()]\n",
        "\n",
        "      # 11.\n",
        "      pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "      pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "      # 12.\n",
        "      end_time = timer()\n",
        "      pred_dict[\"time_for_pred\"] = round(end_time - start_time, 4)\n",
        "\n",
        "    # 13.\n",
        "    pred_dict[\"correct\"] = class_names == pred_class\n",
        "\n",
        "    # 14.\n",
        "    pred_list.append(pred_dict)\n",
        "  # 15. devolve a lista de dicionarios com as predicoes\n",
        "  return pred_list"
      ],
      "metadata": {
        "id": "KqbtGWauqz-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "import torch\n",
        "\n",
        "from PIL import Image\n",
        "from timeit import default_timer as timer\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict\n",
        "\n",
        "# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\n",
        "def pred_and_store(paths: List[pathlib.Path],\n",
        "                   model: torch.nn.Module,\n",
        "                   transform: torchvision.transforms,\n",
        "                   class_names: List[str],\n",
        "                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -> List[Dict]:\n",
        "\n",
        "    # 2. Create an empty list to store prediction dictionaries\n",
        "    pred_list = []\n",
        "\n",
        "    # 3. Loop through target paths\n",
        "    for path in tqdm(paths):\n",
        "\n",
        "        # 4. Create empty dictionary to store prediction information for each sample\n",
        "        pred_dict = {}\n",
        "\n",
        "        # 5. Get the sample path and ground truth class name\n",
        "        pred_dict[\"image_path\"] = path\n",
        "        class_name = path.parent.stem\n",
        "        pred_dict[\"class_name\"] = class_name\n",
        "\n",
        "        # 6. Start the prediction timer\n",
        "        start_time = timer()\n",
        "\n",
        "        # 7. Open image path\n",
        "        img = Image.open(path)\n",
        "\n",
        "        # 8. Transform the image, add batch dimension and put image on target device\n",
        "        transformed_image = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        # 10. Get prediction probability, predicition label and prediction class\n",
        "        with torch.inference_mode():\n",
        "            pred_logit = model(transformed_image) # perform inference on target sample\n",
        "            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n",
        "            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n",
        "            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n",
        "\n",
        "            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)\n",
        "            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n",
        "            pred_dict[\"pred_class\"] = pred_class\n",
        "\n",
        "            # 12. End the timer and calculate time per pred\n",
        "            end_time = timer()\n",
        "            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n",
        "\n",
        "        # 13. Does the pred match the true label?\n",
        "        pred_dict[\"correct\"] = class_name == pred_class\n",
        "\n",
        "        # 14. Add the dictionary to the list of preds\n",
        "        pred_list.append(pred_dict)\n",
        "\n",
        "    # 15. Return list of prediction dictionaries\n",
        "    return pred_list"
      ],
      "metadata": {
        "id": "IFgwDtQb1Hvf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.2 Realizando as predicoes e conometrando-as\n",
        "\n",
        "Usando a `pred_and_store()`\n",
        "\n",
        "Pontos chaves:\n",
        "1. O dispositivo sera CPU, configurada manualmente (Pq nao se tem certeza que sempre havera uma GPU quando o medelo entrar em deploy)\n",
        "2. Transforms - os modelo tem que estarem realizando as predicoes com os dados de teste que foram transformados para os repectivos modelos."
      ],
      "metadata": {
        "id": "KFyzR4s6raGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### EffNetB2"
      ],
      "metadata": {
        "id": "43ECc3GDxdT_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=effnetb2,\n",
        "                                          transform=effnetb2_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\")"
      ],
      "metadata": {
        "id": "4sXdxG5Dt3DJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_test_pred_dicts[:2]\n"
      ],
      "metadata": {
        "id": "2tRNyHbqudoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TRansformando o test_pred_dicts em um DataFrame\n",
        "import pandas as pd\n",
        "effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\n",
        "effnetb2_test_pred_df.head()"
      ],
      "metadata": {
        "id": "Drk0WiAYufWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ChecaNDO O NUMERO DE PREVISOES CORRETAS\n",
        "effnetb2_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "3OR1RBJ1u0IU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tempo medio de predicao\n",
        "effnetb2_average_time_per_pred= round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"Tempo medio de predicao da EffNetB2: {effnetb2_average_time_per_pred}\")\n"
      ],
      "metadata": {
        "id": "Np5Z0obUvFPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\n",
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "sEXtTcQcUYnr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ViT"
      ],
      "metadata": {
        "id": "k_RnDeNyxZ7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n",
        "                                          model=vit,\n",
        "                                          transform=vit_transforms,\n",
        "                                          class_names=class_names,\n",
        "                                          device=\"cpu\")\n",
        "vit_test_pred_dicts[:2]"
      ],
      "metadata": {
        "id": "L-8GXG_gxbYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\n",
        "vit_test_pred_df.head()"
      ],
      "metadata": {
        "id": "Nin-lwosxmYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#ChecaNDO O NUMERO DE PREVISOES CORRETAS\n",
        "vit_test_pred_df.correct.value_counts()"
      ],
      "metadata": {
        "id": "YULOqEErxmA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tempo medio de predicao\n",
        "vit_average_time_per_pred= round(vit_test_pred_df.time_for_pred.mean(), 4)\n",
        "print(f\"Tempo medio de predicao da ViT: {vit_average_time_per_pred} \")"
      ],
      "metadata": {
        "id": "NlLvjITSxyrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add average prediction time for ViT model on CPU\n",
        "vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\n",
        "vit_stats"
      ],
      "metadata": {
        "id": "Ja4syX9zzJhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2_stats"
      ],
      "metadata": {
        "id": "7ufrclvgIf4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Comparando os resultados dos modelos"
      ],
      "metadata": {
        "id": "NgFM-PrxV-xk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#transformando os dict em dataframws\n",
        "df = pd.DataFrame([effnetb2_stats, vit_stats])\n",
        "\n",
        "#Add uma coluna de nome\n",
        "df[\"model\"] = ['EffNetB2', \"ViT\"]\n",
        "\n",
        "#Converte acc em porcentagem\n",
        "df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)"
      ],
      "metadata": {
        "id": "hvM-F_5wWDf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "A9NnQ4RBIV3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Qual modelo eh o melhor?\n",
        "* test_loss (menos e melhor) -ViT\n",
        "* test_acc (mais e melhor) -ViT\n",
        "* number_of_parameters (geralmente menor e maior*) - se um modelo tem mais parametros, geralmente demora mais para computr\n",
        "  * *as vezes modelos com mais parametros ainda conseguem computar mais rapido\n",
        "* model_size (MB) - EffNetB2 (PAra o nosso caso que o deploy sera para mobile, geralmente menor eh melhor)\n",
        "* time_per_pred_cpu (menos e melhor, sera altamente dependendo do hardware que esta rodando\n",
        "\n",
        "Os dois modelos falharam em atingir o objetivo de 30+FPS... Com tudo se pode so tenatr e usar o EffNetB2 e ver como ele se sai"
      ],
      "metadata": {
        "id": "wsNIExi0Wohf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Comprando ViT com EffNetB2\n",
        "pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]),\n",
        "             columns=[\"ViT to EffNetB2 ratios\"]).T"
      ],
      "metadata": {
        "id": "IM7kdioeXM-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.1 Visualizando o tradeoff Velocidade X Performace\n",
        "\n",
        "PAra comparar os dois features extrator de maneira visual:\n",
        "\n",
        "1. Criar um scatter plot usando o datafrade para comprar a acc de teste e o tempo de predicao entre EffNetB2 X ViT.\n",
        "2. Add titulos e rotulos pra grafico ficar mais legivel.\n",
        "3. Anotar os amostras no grafico\n",
        "4. Criar a legende para o grafico com base no tamanho dos modelos (MB)"
      ],
      "metadata": {
        "id": "XzrYTQ8TYNdp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Criar grafico para comparacao do DataFrame\n",
        "import matplotlib.pyplot as plt\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "scatter = ax.scatter(data=df,\n",
        "                     x=\"time_per_pred_cpu\",\n",
        "                     y=\"test_acc\",\n",
        "                     c=[\"blue\", \"orange\"],\n",
        "                     s=\"model_size (MB)\")\n",
        "\n",
        "# 2. Titulos e labels\n",
        "ax.set_title(\"FoodVision Mini\", fontsize=18)\n",
        "ax.set_xlabel(\"TEmpo de predicao por imagem em segundos\", fontsize=14)\n",
        "ax.set_ylabel(\"TEste Acc\", fontsize=14)\n",
        "ax.tick_params(axis=\"both\", labelsize=12)\n",
        "ax.grid(True)\n",
        "\n",
        "# 3.\n",
        "for index, row in df.iterrows():\n",
        "  ax.annotate(text=row[\"model\"],\n",
        "              xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n",
        "              size=12)\n",
        "\n",
        "# 4. Criando as legendas baseada nos tamanho dos modelos\n",
        "handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\n",
        "model_size_legend = ax.legend(handles,\n",
        "                              labels,\n",
        "                              loc=\"lower right\",\n",
        "                              fontsize=12)\n",
        "\n",
        "\n",
        "plt.savefig(\"09-foodvision-mini-inference-speed-vs-performance.jpg\")\n",
        "\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "CwUlOxdbcva0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Dando vida ao FoodVision Mini Usando uma demo do Gradio\n",
        "\n",
        "EffNetB2 foi escolhido com base nos criterio"
      ],
      "metadata": {
        "id": "Y_ujI2vLbxUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import/install Gradio\n",
        "try:\n",
        "    import gradio as gr\n",
        "except:\n",
        "    !pip -q install gradio\n",
        "    import gradio as gr\n",
        "\n",
        "print(f\"Gradio version: {gr.__version__}\")"
      ],
      "metadata": {
        "id": "MDL5rZG1H5Ig"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.1 Gradio Overview\n",
        "\n",
        "PQ usar? Pra criar um mvp do modelo que possa ser testado por outras pessoas.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "inputs -> fucntion/model -> outputs\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "985dkf0RKAEz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.2  Criando uma funcao para mapear nossas entradas e saidas"
      ],
      "metadata": {
        "id": "p7ljqG7fKWgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "effnetb2 = effnetb2.to(\"cpu\")\n",
        "\n",
        "#Checando se foi para o dispositivo\n",
        "next(iter(effnetb2.parameters())).device"
      ],
      "metadata": {
        "id": "G9Ea5cejI6Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A funcao `predict()` ira fazer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "Imagem de comida -> Modelo (EffNetB2) -> [Classe da comida e tempo de predicao]\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "XhxvHX7JRHkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, Tuple\n",
        "\n",
        "def predict(img) -> Tuple[Dict, float]:\n",
        "\n",
        "  #Comeca o time\n",
        "  start_time = timer()\n",
        "\n",
        "  #Transforma a imagem para o padrao EffNetB2\n",
        "  img = effnetb2_transforms(img).unsqueeze(0) #unqueeze = add uma dimensao batch na posicao 0\n",
        "\n",
        "  #Colocando o model em eval\n",
        "  effnetb2.eval()\n",
        "  with torch.inference_mode():\n",
        "\n",
        "    #Pasando a imagem transformada pelo modelo de devolvendo os logist em pred prob\n",
        "    pred_prob = torch.softmax(effnetb2(img), dim=1)\n",
        "\n",
        "  # Criando a label de predicao e o dicionario\n",
        "  pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n",
        "\n",
        "  #Calculando o tempo de predicoa\n",
        "  end_time = timer()\n",
        "  pred_time = round(end_time - start_time, 4)\n",
        "\n",
        "  return pred_labels_and_probs, pred_time"
      ],
      "metadata": {
        "id": "SYngav2sRZvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "#Pega em lista todos os caminhos da pasta test\n",
        "test_data_path = list(Path(test_dir).glob(\"*/*.jpg\"))\n",
        "test_data_path\n",
        "\n",
        "#Selecionando a imagem de test aleatoriamente\n",
        "random_image_path = random.sample(test_data_paths, k=1)[0]\n",
        "random_image_path\n",
        "\n",
        "#Open the target image\n",
        "image = Image.open(random_image_path)\n",
        "print(f\"[INFO] Predizendo a imagem de caminho: {random_image_path}\\n\")\n",
        "\n",
        "#Prevendo e mostrando o dict e tempo\n",
        "pred_dict, pred_time = predict(img=image)\n",
        "print(pred_dict)\n",
        "print(pred_time)"
      ],
      "metadata": {
        "id": "QFanhDzTRqvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7.3 Criando uma lista de exemplos de Imagens"
      ],
      "metadata": {
        "id": "dnSu3WBAmmGf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Criando a lista de exemplos\n",
        "exemple_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\n",
        "exemple_list"
      ],
      "metadata": {
        "id": "ITU-H1L_RqIO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}